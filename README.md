# Entropy (information theory)<br>

Information entropy is a concept from information theory. It tells how much information there is in an event. In general, the more certain or deterministic the event is, the less information it will contain. More clearly stated, information is an increase in uncertainty or entropy.<br>

![5a9e707eb6c54290db4ee6be05582944e34e30c8](https://user-images.githubusercontent.com/66637696/189416915-91bda3c0-bfea-474c-8553-a3f806291fb7.svg)


https://github.com/PhillKroger/CodeIQ/blob/master/The%20Most%20Important%20Implementations/The_Shannon_Entropy.py
